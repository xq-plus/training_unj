{"cells":[{"cell_type":"markdown","id":"a786c77c","metadata":{"id":"a786c77c"},"source":["# LangChain: Memory\n","\n","## Outline\n","* ConversationBufferMemory: Like a chat log that records everything in the conversation so far.\n","* ConversationBufferWindowMemory: only remembers the most recent part of your chat, not the entire conversation.\n","* ConversationTokenBufferMemory: Instead of remembering by a number of phrases or sentences, it remembers based on a certain amount of space it can hold, called “tokens.”\n","* ConversationSummaryMemory: Instead of keeping track of every word, it writes down short summaries of important parts of the conversation"]},{"cell_type":"markdown","source":["## Install Modules"],"metadata":{"id":"LhyspRJch4su"},"id":"LhyspRJch4su"},{"cell_type":"code","source":["!pip install litellm"],"metadata":{"id":"MwogkJyofqzp"},"id":"MwogkJyofqzp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain"],"metadata":{"id":"T4ihJsUlf0Ox"},"id":"T4ihJsUlf0Ox","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_community"],"metadata":{"id":"siYWXwbMf1J8"},"id":"siYWXwbMf1J8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_openai"],"metadata":{"id":"ITBYIF_mf2MQ"},"id":"ITBYIF_mf2MQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1297dcd5","metadata":{"id":"1297dcd5"},"source":["## ConversationBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"a1f518f5","metadata":{"height":132,"id":"a1f518f5"},"outputs":[],"source":["import os\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"id":"20ad6fe2","metadata":{"height":81,"id":"20ad6fe2"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"88bdf13d","metadata":{"height":132,"id":"88bdf13d"},"outputs":[],"source":["# Set up a conversational model with a buffer memory to track full conversation history\n","llm = ChatOpenAI(model=\"qwen2.5\",\n","                 api_key=\"\",\n","                 base_url='',\n","                 temperature=0.0)\n","\n","# Initialize memory and conversation chain\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"db24677d","metadata":{"height":30,"id":"db24677d"},"outputs":[],"source":["# Simulate conversation inputs\n","conversation.predict(input=\"Hi, my name is Andrew\")"]},{"cell_type":"code","execution_count":null,"id":"cc3ef937","metadata":{"height":30,"id":"cc3ef937"},"outputs":[],"source":["conversation.predict(input=\"What is 1+1?\")"]},{"cell_type":"code","execution_count":null,"id":"acf3339a","metadata":{"height":30,"id":"acf3339a"},"outputs":[],"source":["conversation.predict(input=\"What is my name?\")"]},{"cell_type":"code","execution_count":null,"id":"2529400d","metadata":{"height":30,"id":"2529400d"},"outputs":[],"source":["# Display entire conversation history stored in the buffer\n","print(memory.buffer)"]},{"cell_type":"code","execution_count":null,"id":"5018cb0a","metadata":{"height":30,"id":"5018cb0a"},"outputs":[],"source":["# Demonstrate saving and loading conversation states\n","memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"14219b70","metadata":{"height":30,"id":"14219b70"},"outputs":[],"source":["memory = ConversationBufferMemory()"]},{"cell_type":"code","execution_count":null,"id":"a36e9905","metadata":{"height":47,"id":"a36e9905"},"outputs":[],"source":["memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})"]},{"cell_type":"code","execution_count":null,"id":"61631b1f","metadata":{"height":30,"id":"61631b1f"},"outputs":[],"source":["print(memory.buffer)"]},{"cell_type":"code","execution_count":null,"id":"a2fdf9ec","metadata":{"height":30,"id":"a2fdf9ec"},"outputs":[],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"7ca79256","metadata":{"height":47,"id":"7ca79256"},"outputs":[],"source":["memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})"]},{"cell_type":"code","execution_count":null,"id":"890a4497","metadata":{"height":30,"id":"890a4497"},"outputs":[],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"cf98e9ff","metadata":{"id":"cf98e9ff"},"source":["## ConversationBufferWindowMemory"]},{"cell_type":"code","execution_count":null,"id":"66eeccc3","metadata":{"height":30,"id":"66eeccc3"},"outputs":[],"source":["from langchain.memory import ConversationBufferWindowMemory"]},{"cell_type":"code","execution_count":null,"id":"3ea6233e","metadata":{"height":30,"id":"3ea6233e"},"outputs":[],"source":["# Initialize memory with a fixed window (k=1) to retain only the latest interaction\n","memory = ConversationBufferWindowMemory(k=1)"]},{"cell_type":"code","execution_count":null,"id":"dc4553fb","metadata":{"height":98,"id":"dc4553fb"},"outputs":[],"source":["# Store recent conversation interactions within the set window size\n","memory.save_context({\"input\": \"Hi\"},\n","                    {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})"]},{"cell_type":"code","execution_count":null,"id":"6a788403","metadata":{"height":30,"id":"6a788403"},"outputs":[],"source":["memory.load_memory_variables({})  # Load only the latest context due to window size"]},{"cell_type":"code","execution_count":null,"id":"4087bc87","metadata":{"height":132,"id":"4087bc87"},"outputs":[],"source":["# Verify the memory’s windowed effect in a conversation\n","memory = ConversationBufferWindowMemory(k=1)\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"id":"4faaa952","metadata":{"height":30,"id":"4faaa952"},"outputs":[],"source":["conversation.predict(input=\"Hi, my name is Andrew\")"]},{"cell_type":"code","execution_count":null,"id":"bb20ddaa","metadata":{"height":30,"id":"bb20ddaa"},"outputs":[],"source":["conversation.predict(input=\"What is 1+1?\")"]},{"cell_type":"code","execution_count":null,"id":"489b2194","metadata":{"height":30,"id":"489b2194"},"outputs":[],"source":["conversation.predict(input=\"What is my name?\")"]},{"cell_type":"markdown","id":"d2931b92","metadata":{"id":"d2931b92"},"source":["## ConversationTokenBufferMemory"]},{"cell_type":"markdown","source":["*Skipped during simulation due to the model Qwen2.5 being not supported by the tokenizer library tiktoken*"],"metadata":{"id":"crD_9f1gWsZ1"},"id":"crD_9f1gWsZ1"},{"cell_type":"code","execution_count":null,"id":"fb9020ed","metadata":{"height":64,"id":"fb9020ed"},"outputs":[],"source":["from langchain_community.chat_models import ChatLiteLLM\n","from langchain.memory import ConversationTokenBufferMemory\n","from langchain.llms import OpenAI"]},{"cell_type":"code","source":["from langchain.memory import ConversationTokenBufferMemory\n","from langchain.chat_models import ChatInhouse\n","llm = ChatInhouse(model=\"qwen2.5\",\n","                 api_key=\"sk-68ZEzMn9vg_c6SWQhBZMYg\",\n","                 base_url='https://litellm.level-up.co.id/',\n","                 temperature=0.0)"],"metadata":{"id":"PJJdm7KyNnK6"},"id":"PJJdm7KyNnK6","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"43582ee6","metadata":{"height":132,"id":"43582ee6"},"outputs":[],"source":["memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n","\n","memory.save_context({\"input\": \"AI is what?!\"},\n","                    {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"},\n","                    {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"},\n","                    {\"output\": \"Charming!\"})"]},{"cell_type":"code","execution_count":null,"id":"284288e1","metadata":{"height":30,"id":"284288e1"},"outputs":[],"source":["memory.load_memory_variables({})"]},{"cell_type":"markdown","id":"5ff55d5d","metadata":{"id":"5ff55d5d"},"source":["## ConversationSummaryMemory"]},{"cell_type":"code","execution_count":null,"id":"72dcf8b1","metadata":{"height":47,"id":"72dcf8b1"},"outputs":[],"source":["from langchain.memory import ConversationSummaryBufferMemory"]},{"cell_type":"code","execution_count":null,"id":"4a5b238f","metadata":{"height":268,"id":"4a5b238f"},"outputs":[],"source":["# Create sample context with a long message\n","schedule = \"There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n","\n","# Initialize memory with summary-based retention (condenses prior exchanges into summaries)\n","memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n","\n","# Save initial context and more complex interactions\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"},\n","                    {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"},\n","                    {\"output\": f\"{schedule}\"})"]},{"cell_type":"code","execution_count":null,"id":"2e4ecabe","metadata":{"height":30,"id":"2e4ecabe"},"outputs":[],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"6728edba","metadata":{"height":98,"id":"6728edba"},"outputs":[],"source":["# Engage in conversation using summarized memory\n","conversation = ConversationChain(\n","    llm=llm,\n","    memory = memory,\n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"id":"9a221b1d","metadata":{"height":30,"id":"9a221b1d"},"outputs":[],"source":["conversation.predict(input=\"What would be a good demo to show?\")"]},{"cell_type":"code","execution_count":null,"id":"bb582617","metadata":{"height":30,"id":"bb582617"},"outputs":[],"source":["memory.load_memory_variables({})"]},{"cell_type":"code","execution_count":null,"id":"08ed59c4","metadata":{"height":30,"id":"08ed59c4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[],"collapsed_sections":["LhyspRJch4su","d2931b92"]}},"nbformat":4,"nbformat_minor":5}